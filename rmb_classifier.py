import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import os
import cv2
from sklearn.preprocessing import StandardScaler
import pickle
import warnings
warnings.filterwarnings('ignore')


# è®¾ç½®å‚æ•°
IMG_SIZE = 224
BATCH_SIZE = 64  # å¢å¤§batch size
EPOCHS = 50      # å¢åŠ è®­ç»ƒè½®æ•°
DATA_DIR = "./RMB-Dataset/RMBDataset"  # æ•°æ®é›†è·¯å¾„,è®°å¾—åœ¨æœ¬ç›®å½•ä¸‹æ‰§è¡Œæ–‡ä»¶
MODEL_SAVE_PATH = 'rmb_classifier_with_features.h5'
FEATURES_SAVE_PATH = 'feature_extractor.pkl'

# é«˜çº§è®­ç»ƒå‚æ•°
TRAINING_PARAMS = {
    # ä¼˜åŒ–å™¨å‚æ•°
    'optimizer': 'adam',  # 'adam', 'sgd', 'rmsprop'
    'learning_rate': 0.001,
    'weight_decay': 1e-4,  # L2æ­£åˆ™åŒ–

    # å­¦ä¹ ç‡è°ƒåº¦
    'lr_scheduler': 'reduce_on_plateau',  # 'reduce_on_plateau', 'cosine', 'step', None
    'lr_factor': 0.5,
    'lr_patience': 10,
    'lr_min': 1e-7,

    # æ—©åœæœºåˆ¶
    'early_stopping': True,
    'early_stopping_patience': 15,
    'early_stopping_monitor': 'val_accuracy',

    # æ¨¡å‹æ£€æŸ¥ç‚¹
    'checkpoint': True,
    'checkpoint_monitor': 'val_accuracy',

    # æ•°æ®å¢å¼ºï¼ˆç‰¹å¾ç©ºé—´ï¼‰
    'feature_noise': 0.01,  # æ·»åŠ å™ªå£°å¢å¼º
    'feature_dropout': 0.1,  # ç‰¹å¾dropout
}


class FeatureExtractor:
    """ä»qt_camera.pyç§»æ¤çš„ç‰¹å¾æå–ç±»"""

    @staticmethod
    def extract_features(image):
        """æå–çº¸å¸çš„æ‰€æœ‰ç‰¹å¾"""
        # ç¡®ä¿å›¾åƒæ˜¯CV2æ ¼å¼ï¼ˆBGRï¼‰
        if isinstance(image, tf.Tensor):
            image = image.numpy()
            if image.max() <= 1.0:
                image = (image * 255).astype(np.uint8)
            if len(image.shape) == 4:
                image = image[0]  # å»æ‰batchç»´åº¦
            if len(image.shape) == 3:
                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)

        # 1. Harrisè§’ç‚¹æ£€æµ‹
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        gray_float = np.float32(gray)
        dst = cv2.cornerHarris(gray_float, blockSize=2, ksize=3, k=0.04)
        dst = cv2.dilate(dst, None)
        harris_corners = np.sum(dst > 0.01 * dst.max())

        # è½¬æ¢ä¸ºuint8ï¼ˆShi-Tomasiã€ORBã€Cannyéœ€è¦ï¼‰
        gray_uint8 = gray.astype(np.uint8)

        # 2. Shi-Tomasiè§’ç‚¹æ£€æµ‹
        corners_st = cv2.goodFeaturesToTrack(gray_uint8, maxCorners=100, qualityLevel=0.01, minDistance=10)
        shi_tomasi_corners = len(corners_st) if corners_st is not None else 0

        # 3. ORBç‰¹å¾ç‚¹æ£€æµ‹
        orb = cv2.ORB_create(nfeatures=1000)
        keypoints, descriptors = orb.detectAndCompute(gray_uint8, None)
        orb_features = len(keypoints)

        # 4. è½®å»“æ£€æµ‹å’Œå‡ ä½•ç‰¹å¾
        edges = cv2.Canny(gray_uint8, 50, 150)
        contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        aspect_ratio = 0
        area_ratio = 0

        if contours:
            largest_contour = max(contours, key=cv2.contourArea)
            # è®¡ç®—é•¿å®½æ¯”
            rect = cv2.boundingRect(largest_contour)
            x, y, w, h = rect
            aspect_ratio = max(w, h) / min(w, h) if min(w, h) > 0 else 0

            # è®¡ç®—é¢ç§¯å æ¯”
            frame_area = image.shape[0] * image.shape[1]
            contour_area = cv2.contourArea(largest_contour)
            area_ratio = contour_area / frame_area if frame_area > 0 else 0

        # 5. Cannyè¾¹ç¼˜ç»Ÿè®¡
        edges_count = np.sum(edges > 0)

        # 6. é¢œè‰²ç›´æ–¹å›¾ç‰¹å¾ï¼ˆç®€åŒ–ç‰ˆï¼‰
        hist_b = cv2.calcHist([image], [0], None, [32], [0, 256])
        hist_g = cv2.calcHist([image], [1], None, [32], [0, 256])
        hist_r = cv2.calcHist([image], [2], None, [32], [0, 256])

        # ç»„åˆæ‰€æœ‰ç‰¹å¾
        features = np.array([
            harris_corners,
            shi_tomasi_corners,
            orb_features,
            aspect_ratio,
            area_ratio,
            edges_count,
            *hist_b.flatten(),
            *hist_g.flatten(),
            *hist_r.flatten()
        ])

        return features

    @staticmethod
    def extract_denomination_scores(features):
        """åŸºäºç‰¹å¾é¢„æµ‹é¢é¢ï¼ˆä»qt_camera.pyç§»æ¤çš„predict_denominationé€»è¾‘ï¼‰"""
        if len(features) < 6:
            return "æœªçŸ¥", {}

        harris_corners = features[0]
        shi_tomasi_corners = features[1]
        orb_features = features[2]
        aspect_ratio = features[3]
        area_ratio = features[4]

        # äººæ°‘å¸å„é¢é¢çš„å…¸å‹ç‰¹å¾
        denomination_features = {
            '1': {
                'aspect_ratio_range': (1.5, 1.8),
                'area_ratio_range': (0.15, 0.25),
                'corner_range': (15, 40),
                'feature_range': (50, 150)
            },
            '5': {
                'aspect_ratio_range': (1.5, 1.8),
                'area_ratio_range': (0.15, 0.25),
                'corner_range': (15, 40),
                'feature_range': (50, 150)
            },
            '10': {
                'aspect_ratio_range': (1.7, 2.0),
                'area_ratio_range': (0.20, 0.30),
                'corner_range': (20, 50),
                'feature_range': (80, 200)
            },
            '20': {
                'aspect_ratio_range': (1.7, 2.0),
                'area_ratio_range': (0.20, 0.30),
                'corner_range': (20, 50),
                'feature_range': (80, 200)
            },
            '50': {
                'aspect_ratio_range': (1.8, 2.1),
                'area_ratio_range': (0.25, 0.35),
                'corner_range': (25, 60),
                'feature_range': (100, 250)
            },
            '100': {
                'aspect_ratio_range': (1.8, 2.1),
                'area_ratio_range': (0.25, 0.35),
                'corner_range': (25, 60),
                'feature_range': (100, 250)
            }
        }

        # è®¡ç®—åŒ¹é…åˆ†æ•°
        scores = {}
        for denom, feat_ranges in denomination_features.items():
            score = 0

            # é•¿å®½æ¯”åŒ¹é…
            ar_min, ar_max = feat_ranges['aspect_ratio_range']
            if ar_min <= aspect_ratio <= ar_max:
                score += 25

            # é¢ç§¯å æ¯”åŒ¹é…
            area_min, area_max = feat_ranges['area_ratio_range']
            if area_min <= area_ratio <= area_max:
                score += 25

            # è§’ç‚¹æ•°é‡åŒ¹é…
            corner_min, corner_max = feat_ranges['corner_range']
            if corner_min <= shi_tomasi_corners <= corner_max:
                score += 25

            # ç‰¹å¾ç‚¹æ•°é‡åŒ¹é…
            feat_min, feat_max = feat_ranges['feature_range']
            if feat_min <= orb_features <= feat_max:
                score += 25

            scores[denom] = score

        # é€‰æ‹©åˆ†æ•°æœ€é«˜çš„é¢é¢
        if scores:
            best_denomination = max(scores, key=scores.get)
            best_score = scores[best_denomination]

            if best_score < 40:
                return "æœªçŸ¥", scores

            return best_denomination, scores

        return "æœªçŸ¥", {}

def load_and_extract_features():
    """åŠ è½½æ•°æ®å¹¶æå–ç‰¹å¾"""
    data_dir = os.path.abspath(DATA_DIR)

    # åŠ è½½æ‰€æœ‰å›¾åƒæ•°æ®ï¼ˆä¸ä½¿ç”¨batchï¼‰
    all_data = tf.keras.utils.image_dataset_from_directory(
        data_dir,
        image_size=(IMG_SIZE, IMG_SIZE),
        batch_size=None,
        shuffle=False
    )

    class_names = all_data.class_names
    print(f"ç±»åˆ«: {class_names}")
    print(f"æ€»ç±»åˆ«æ•°: {len(class_names)}")

    # åˆ†ç¦»å›¾åƒå’Œæ ‡ç­¾
    images = []
    labels = []

    print("æ­£åœ¨æå–å›¾åƒç‰¹å¾...")
    for image, label in all_data:
        # æå–ç‰¹å¾
        features = FeatureExtractor.extract_features(image.numpy())
        images.append(features)
        labels.append(label.numpy())

        # è¿›åº¦æ˜¾ç¤º
        if len(images) % 50 == 0:
            print(f"å·²å¤„ç†: {len(images)} å¼ å›¾åƒ")

    X = np.array(images)
    y = np.array(labels)

    print(f"ç‰¹å¾æå–å®Œæˆï¼")
    print(f"ç‰¹å¾ç»´åº¦: {X.shape}")
    print(f"æ ‡ç­¾ç»´åº¦: {y.shape}")

    # ç‰¹å¾æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    return X_scaled, y, class_names, scaler


def split_data(X, y, test_size=0.2, random_state=42):
    """åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›†"""
    from sklearn.model_selection import train_test_split

    X_train, X_val, y_train, y_val = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=y
    )

    return X_train, X_val, y_train, y_val

def create_feature_based_model(input_dim, num_classes):
    """åˆ›å»ºåŸºäºç‰¹å¾çš„æ¨¡å‹ï¼ˆç”¨äºå¤„ç†æå–çš„ç‰¹å¾ï¼‰"""
    # è·å–è®­ç»ƒå‚æ•°
    params = TRAINING_PARAMS

    # é€‰æ‹©ä¼˜åŒ–å™¨
    if params['optimizer'] == 'adam':
        optimizer = keras.optimizers.Adam(
            learning_rate=params['learning_rate'],
            weight_decay=params['weight_decay']
        )
    elif params['optimizer'] == 'sgd':
        optimizer = keras.optimizers.SGD(
            learning_rate=params['learning_rate'],
            momentum=0.9,
            weight_decay=params['weight_decay']
        )
    elif params['optimizer'] == 'rmsprop':
        optimizer = keras.optimizers.RMSprop(
            learning_rate=params['learning_rate'],
            weight_decay=params['weight_decay']
        )
    else:
        optimizer = 'adam'

    # åˆ›å»ºæ¨¡å‹ï¼ˆæ›´æ·±å±‚ï¼Œæ·»åŠ æ›´å¤šæ­£åˆ™åŒ–ï¼‰
    model = keras.Sequential([
        layers.InputLayer(input_shape=(input_dim,)),
        layers.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(params['weight_decay'])),
        layers.BatchNormalization(),
        layers.Dropout(0.4),
        layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(params['weight_decay'])),
        layers.BatchNormalization(),
        layers.Dropout(0.4),
        layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(params['weight_decay'])),
        layers.BatchNormalization(),
        layers.Dropout(0.3),
        layers.Dense(64, activation='relu', kernel_regularizer=keras.regularizers.l2(params['weight_decay'])),
        layers.Dropout(0.2),
        layers.Dense(num_classes, activation='softmax')
    ])

    model.compile(
        optimizer=optimizer,
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    return model


def create_cnn_model(num_classes):
    """åˆ›å»ºCNNæ¨¡å‹ï¼ˆç”¨äºå¯¹æ¯”ï¼‰"""
    model = keras.Sequential([
        layers.Rescaling(1./255, input_shape=(IMG_SIZE, IMG_SIZE, 3)),
        layers.Conv2D(32, 3, activation='relu'),
        layers.MaxPooling2D(),
        layers.Conv2D(64, 3, activation='relu'),
        layers.MaxPooling2D(),
        layers.Conv2D(128, 3, activation='relu'),
        layers.MaxPooling2D(),
        layers.Flatten(),
        layers.Dense(256, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation='softmax')
    ])

    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

def get_training_callbacks():
    """è·å–è®­ç»ƒå›è°ƒå‡½æ•°"""
    params = TRAINING_PARAMS
    callbacks = []

    # æ—©åœ
    if params['early_stopping']:
        early_stopping = keras.callbacks.EarlyStopping(
            monitor=params['early_stopping_monitor'],
            patience=params['early_stopping_patience'],
            restore_best_weights=True,
            verbose=1
        )
        callbacks.append(early_stopping)

    # å­¦ä¹ ç‡è°ƒåº¦ - ReduceLROnPlateau
    if params['lr_scheduler'] == 'reduce_on_plateau':
        lr_scheduler = keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=params['lr_factor'],
            patience=params['lr_patience'],
            min_lr=params['lr_min'],
            verbose=1
        )
        callbacks.append(lr_scheduler)

    # ä½™å¼¦é€€ç«è°ƒåº¦ï¼ˆå¯é€‰ï¼‰
    elif params['lr_scheduler'] == 'cosine':
        # ä½¿ç”¨CosineDecayï¼Œéœ€è¦å…ˆè®¡ç®—æ€»æ­¥æ•°
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æ ¹æ®æ•°æ®é‡è®¡ç®—
        pass

    # æ¨¡å‹æ£€æŸ¥ç‚¹
    if params['checkpoint']:
        checkpoint = keras.callbacks.ModelCheckpoint(
            filepath=MODEL_SAVE_PATH.replace('.h5', '_best.h5'),
            monitor=params['checkpoint_monitor'],
            save_best_only=True,
            verbose=1
        )
        callbacks.append(checkpoint)

    return callbacks


def train_feature_based_model():
    """è®­ç»ƒåŸºäºç‰¹å¾çš„æ¨¡å‹"""
    print("=" * 50)
    print("è®­ç»ƒåŸºäºç‰¹å¾çš„æ¨¡å‹")
    print("=" * 50)

    # æ˜¾ç¤ºè®­ç»ƒå‚æ•°
    params = TRAINING_PARAMS
    print("\nè®­ç»ƒå‚æ•°:")
    print(f"  ä¼˜åŒ–å™¨: {params['optimizer']}")
    print(f"  å­¦ä¹ ç‡: {params['learning_rate']}")
    print(f"  L2æ­£åˆ™åŒ–: {params['weight_decay']}")
    print(f"  æ—©åœ: {params['early_stopping']}")
    print(f"  å­¦ä¹ ç‡è°ƒåº¦: {params['lr_scheduler']}")
    print("")

    # åŠ è½½æ•°æ®å¹¶æå–ç‰¹å¾
    X, y, class_names, scaler = load_and_extract_features()

    # åˆ†å‰²æ•°æ®
    X_train, X_val, y_train, y_val = split_data(X, y)

    print(f"\nè®­ç»ƒé›†å¤§å°: {X_train.shape}")
    print(f"éªŒè¯é›†å¤§å°: {X_val.shape}")

    # æ„å»ºæ¨¡å‹
    print("\næ„å»ºç‰¹å¾æ¨¡å‹...")
    model = create_feature_based_model(X_train.shape[1], len(class_names))
    model.summary()

    # è·å–å›è°ƒå‡½æ•°
    callbacks = get_training_callbacks()

    # è®­ç»ƒæ¨¡å‹
    print("\nå¼€å§‹è®­ç»ƒç‰¹å¾æ¨¡å‹...")
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        callbacks=callbacks,
        verbose=1  # æ˜¾ç¤ºè®­ç»ƒè¿›åº¦
    )

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    model.save(MODEL_SAVE_PATH)
    print(f"\nâœ“ æ¨¡å‹å·²ä¿å­˜ä¸º {MODEL_SAVE_PATH}")

    # ä¿å­˜ç‰¹å¾æ ‡å‡†åŒ–å™¨
    with open(FEATURES_SAVE_PATH, 'wb') as f:
        pickle.dump(scaler, f)
    print(f"âœ“ ç‰¹å¾æ ‡å‡†åŒ–å™¨å·²ä¿å­˜ä¸º {FEATURES_SAVE_PATH}")

    # æ‰“å°æœ€ä½³ç»“æœ
    best_val_acc = max(history.history['val_accuracy'])
    best_epoch = np.argmax(history.history['val_accuracy']) + 1
    print(f"\nğŸ† æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f} (ç¬¬{best_epoch}è½®)")

    return model, history, class_names


def train_cnn_model():
    """è®­ç»ƒCNNæ¨¡å‹ï¼ˆå¯¹æ¯”ç”¨ï¼‰"""
    print("=" * 50)
    print("è®­ç»ƒCNNæ¨¡å‹ï¼ˆå¯¹æ¯”ï¼‰")
    print("=" * 50)

    # é‡æ–°åŠ è½½åŸå§‹å›¾åƒæ•°æ®
    data_dir = os.path.abspath(DATA_DIR)

    train_ds = tf.keras.utils.image_dataset_from_directory(
        data_dir,
        validation_split=0.2,
        subset="training",
        seed=123,
        image_size=(IMG_SIZE, IMG_SIZE),
        batch_size=BATCH_SIZE
    )

    val_ds = tf.keras.utils.image_dataset_from_directory(
        data_dir,
        validation_split=0.2,
        subset="validation",
        seed=123,
        image_size=(IMG_SIZE, IMG_SIZE),
        batch_size=BATCH_SIZE
    )

    class_names = train_ds.class_names
    print(f"\nç±»åˆ«: {class_names}")

    # æ•°æ®é¢„å¤„ç†
    train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)
    val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)

    # æ„å»ºæ¨¡å‹
    print("\næ„å»ºCNNæ¨¡å‹...")
    model = create_cnn_model(len(class_names))
    model.summary()

    # è®­ç»ƒæ¨¡å‹
    print("\nå¼€å§‹è®­ç»ƒCNNæ¨¡å‹...")
    history = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=EPOCHS
    )

    # ä¿å­˜æ¨¡å‹
    model.save('rmb_cnn_model.h5')
    print("\nCNNæ¨¡å‹å·²ä¿å­˜ä¸º rmb_cnn_model.h5")

    return model, history, class_names

def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 60)
    print("RMBé¢é¢è¯†åˆ«ç³»ç»Ÿ - ç‰¹å¾æå–æ¨¡å‹è®­ç»ƒ")
    print("=" * 60)
    print("\nè¯·é€‰æ‹©è®­ç»ƒæ¨¡å¼:")
    print("1. åŸºäºç‰¹å¾çš„æ¨¡å‹ï¼ˆæ¨èï¼‰- ä½¿ç”¨Harrisè§’ç‚¹ã€ORBç‰¹å¾ã€å‡ ä½•ç‰¹å¾ç­‰")
    print("2. CNNæ¨¡å‹ - ä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œ")
    print("3. ä¸¤ç§æ¨¡å‹éƒ½è®­ç»ƒï¼ˆå¯¹æ¯”ï¼‰")

    choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1/2/3): ").strip()

    if choice == "1":
        model, history, class_names = train_feature_based_model()
        print(f"\nâœ“ ç‰¹å¾æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        print(f"âœ“ æ¨¡å‹å¯ä»¥è¯†åˆ«: {class_names}")
        print(f"\næ–‡ä»¶ä¿å­˜:")
        print(f"  - æ¨¡å‹æ–‡ä»¶: {MODEL_SAVE_PATH}")
        print(f"  - ç‰¹å¾æ ‡å‡†åŒ–å™¨: {FEATURES_SAVE_PATH}")

    elif choice == "2":
        model, history, class_names = train_cnn_model()
        print(f"\nâœ“ CNNæ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        print(f"âœ“ æ¨¡å‹å¯ä»¥è¯†åˆ«: {class_names}")

    elif choice == "3":
        print("\næ­£åœ¨è®­ç»ƒä¸¤ç§æ¨¡å‹ï¼Œè¯·ç¨å€™...")
        model1, history1, class_names = train_feature_based_model()
        print("\n\n")
        model2, history2, _ = train_cnn_model()

        print("\n" + "=" * 60)
        print("ä¸¤ç§æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        print(f"âœ“ ç‰¹å¾æ¨¡å‹æ–‡ä»¶: {MODEL_SAVE_PATH}")
        print(f"âœ“ ç‰¹å¾æ ‡å‡†åŒ–å™¨: {FEATURES_SAVE_PATH}")
        print(f"âœ“ CNNæ¨¡å‹æ–‡ä»¶: rmb_cnn_model.h5")
        print("=" * 60)

    else:
        print("æ— æ•ˆé€‰æ‹©ï¼")


if __name__ == "__main__":
    main()


def quick_test_model():
    """å¿«é€Ÿæµ‹è¯•æ¨¡å‹æ€§èƒ½"""
    import matplotlib.pyplot as plt

    print("\n" + "=" * 50)
    print("å¿«é€Ÿæ¨¡å‹æµ‹è¯•")
    print("=" * 50)

    # åŠ è½½æ•°æ®å¹¶æå–ç‰¹å¾
    X, y, class_names, scaler = load_and_extract_features()

    # åˆ†å‰²æ•°æ®
    X_train, X_val, y_train, y_val = split_data(X, y)

    # è®­ç»ƒæ¨¡å‹
    model = create_feature_based_model(X_train.shape[1], len(class_names))

    # ç®€å•è®­ç»ƒå‡ è½®ï¼ˆç”¨äºéªŒè¯ï¼‰
    print("\nè®­ç»ƒæ¨¡å‹è¿›è¡Œå¿«é€Ÿæµ‹è¯•...")
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=10,  # åªè®­ç»ƒ10è½®å¿«é€Ÿæµ‹è¯•
        batch_size=BATCH_SIZE,
        verbose=0
    )

    # è¯„ä¼°æ¨¡å‹
    val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)
    print(f"\nâœ“ éªŒè¯é›†å‡†ç¡®ç‡: {val_acc:.4f}")

    # é¢„æµ‹å‡ ä¸ªæ ·æœ¬
    print("\næ ·æœ¬é¢„æµ‹:")
    for i in range(min(5, len(X_val))):
        pred = model.predict(X_val[i:i+1], verbose=0)
        class_idx = np.argmax(pred[0])
        true_idx = y_val[i]
        confidence = pred[0][class_idx]
        print(f"  æ ·æœ¬ {i+1}: é¢„æµ‹={class_names[class_idx]} (ç½®ä¿¡åº¦={confidence:.2f}), å®é™…={class_names[true_idx]}")

    print("\nå¿«é€Ÿæµ‹è¯•å®Œæˆï¼")

    # å¯è§†åŒ–è®­ç»ƒå†å²ï¼ˆç®€å•ç‰ˆï¼‰
    print("\nè®­ç»ƒå†å²:")
    print(f"  åˆå§‹éªŒè¯å‡†ç¡®ç‡: {history.history['val_accuracy'][0]:.4f}")
    print(f"  æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {history.history['val_accuracy'][-1]:.4f}")
    print(f"  æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {max(history.history['val_accuracy']):.4f}")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 60)
    print("RMBé¢é¢è¯†åˆ«ç³»ç»Ÿ - ç‰¹å¾æå–æ¨¡å‹è®­ç»ƒ")
    print("=" * 60)
    print("\nè¯·é€‰æ‹©è®­ç»ƒæ¨¡å¼:")
    print("1. åŸºäºç‰¹å¾çš„æ¨¡å‹ï¼ˆæ¨èï¼‰- ä½¿ç”¨Harrisè§’ç‚¹ã€ORBç‰¹å¾ã€å‡ ä½•ç‰¹å¾ç­‰")
    print("2. CNNæ¨¡å‹ - ä¼ ç»Ÿå·ç§¯ç¥ç»ç½‘ç»œ")
    print("3. å¿«é€Ÿæµ‹è¯• - è®­ç»ƒ10è½®å¿«é€ŸéªŒè¯æ¨¡å‹æ•ˆæœ")
    print("4. ä¸¤ç§æ¨¡å‹éƒ½è®­ç»ƒï¼ˆå¯¹æ¯”ï¼‰")

    choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1/2/3/4): ").strip()

    if choice == "1":
        model, history, class_names = train_feature_based_model()
        print(f"\nâœ“ ç‰¹å¾æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        print(f"âœ“ æ¨¡å‹å¯ä»¥è¯†åˆ«: {class_names}")
        print(f"\næ–‡ä»¶ä¿å­˜:")
        print(f"  - æ¨¡å‹æ–‡ä»¶: {MODEL_SAVE_PATH}")
        print(f"  - ç‰¹å¾æ ‡å‡†åŒ–å™¨: {FEATURES_SAVE_PATH}")

    elif choice == "2":
        model, history, class_names = train_cnn_model()
        print(f"\nâœ“ CNNæ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        print(f"âœ“ æ¨¡å‹å¯ä»¥è¯†åˆ«: {class_names}")

    elif choice == "3":
        quick_test_model()

    elif choice == "4":
        print("\næ­£åœ¨è®­ç»ƒä¸¤ç§æ¨¡å‹ï¼Œè¯·ç¨å€™...")
        model1, history1, class_names = train_feature_based_model()
        print("\n\n")
        model2, history2, _ = train_cnn_model()

        print("\n" + "=" * 60)
        print("ä¸¤ç§æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        print(f"âœ“ ç‰¹å¾æ¨¡å‹æ–‡ä»¶: {MODEL_SAVE_PATH}")
        print(f"âœ“ ç‰¹å¾æ ‡å‡†åŒ–å™¨: {FEATURES_SAVE_PATH}")
        print(f"âœ“ CNNæ¨¡å‹æ–‡ä»¶: rmb_cnn_model.h5")
        print("=" * 60)

    else:
        print("æ— æ•ˆé€‰æ‹©ï¼")


if __name__ == "__main__":
    main()